---
layout: post
title:  Census 2.0
date:   2016-08-12
categories:
  - Devops
  - System Architecture
  -
tags:
  - Cloud
author: Steve Mactaggart
images:
excerpt:
  In light of the <a href="https://twitter.com/hashtag/CensusFAIL">CensusFail</a> the ABS delivered to us on Tuesday night, the team here at Cevo thought we'd put our blog where our mouths are and put forward a number of different architectures we feel would have delivered had we been involved in the creation of this years Census application.
---

Having not already completed the Census, I’m not 100% sure of what the official requirements are, but I will take a crack at designing a solution from a guesstimate of what they would need.


**Requirements:**

* Ability to validate the ‘unique code’ sent to each household.
* Provide multiple pages of forms, explanatory text, with options for conditional field selection controlling validation rules on following actions.
* Confirmation around submission of the information.
* Ability to segregate the submission of different parts of the form to different datastores
* Ensure that the same unique code cannot submit the form multiple times

**And most importantly**

* __Handle load of over 5 million sessions accessing the service within a short period of time.__

_Disclaimer: I’m sure the actual requirements are more comprehensive and have some gotchas in this, but the thought exercise was completed on a Friday night, watching the footy with a beer in my hand.._

I jokingly suggested during the week that a form on top of a google spreadsheet would have been a solid solution, and there are aspects of this that actually do make sense.

## Design considerations

### Deliver as much content as possible from a CDN

One of the biggest issues (if not the biggest) with the existing Census setups was the inability to cope with the demand requirements of 5million+ households having dinner and then all firing up their browsers within a 45min window of each other.

To reduce the burden on the central control points, ensure the application is delivered to customers from multiple edge locations.  Even if one of these points gets swamped, the overall network will be able to respond to the vast majority of service requests.

### Limit the number of stateful requests

In any large scale system, state is your enemy.  It means doing lookups and regularly running through single point of failure choke points.

In this solution, the need for state can be simplified into 2 key events.  An initial event that validates the unique code is truly unique and valid, and the final submission of all the census results.

### Select a solution that provides horizontal scaleability

Even with offloading content to a CDN, and reducing the state, 5million requests to any solution within a 15min window will be tough unless you have architected to support it.

Ensuring that the key processing units in the system are able to be run in parallel, and the technology chosen to deliver this processing can quickly and easily be turned on to add more parallel streams as needed.

### Create a write-only data store to isolate risk of data breach

The primary purpose of this system was to collect the data, it doesn’t need to do anything complex with it, doesn’t need to put it into any transactional system, it only needs to validate the content is complete, and format of any key items is correct and record it.

Processing to move this data into a transactional system, create reports or analytics, can be done offline, in longer term slower processing approaches.

### Make change to the system easy and consistent

It’s also important to know that you won’t get it right, and when playing at this scale, it’s important that if things do go wrong, that you can easily and consistently make changes to the running system.

I’ve seen lots of comments this week about how companies like Facebook and Twitter can handle billions of users, not just a few million like on census night.  But remember they have had years to perfect their systems, and the only way they handle this scale was through the ability to evolve.

With the census event, this was a big bang event, no chance to trial this on real load, and so a large number of assumptions had to be made, and often assumptions are only proven incorrect in production.

Being able to make changes to running systems in the heat of the battle, with confidence that the changes you are making are not making things worse should be a key component in any application design.

## Solution design

At this scale, you need a service provider who can provide internet scale services, has been battle tested and has a proven track record of handling massive scale applications.

Over the past few year AWS have proven multiple times to fit this bill, and with the introduction of a series of new services that increase flexibilty and reduce the management overhead, feels like a perfect fit for a Census 2.0 application.

**Overview**

By using a Serverless architecture, the application sits on a platform that is built for scale, and forces the application developers to strongly consider the management and impact of state.

<img src="/images/census-sa.png">

**Front end**

Start by developing the Census form in a manor that sees the bulk of the logic and content delivered to the browser as a set of static HTML, CSS and JavaScript.  There are a number of JavaScript based UI toolkits, so I'll leave it to you to pick you favourite.

The only requirements on the front end is that all of logic, content and validation can be delivered from a CDN to the browser without any need for dynamic content.

**CDN**

To deliver this content to customers, use the CDN combination of <a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/MigrateS3ToCloudFront.html">S3 and CloudFront</a> to provide multiple Edge Locations in both Melbourne and Sydney.

**Security**

Importantly both API Gateway and S3 supports the use of SSL/TLS certificates to ensure security around the delivery of the application to the customers, as well as the delivery of content back to the ABS.

With the class of information being managed by the Census, security is a must, so ensuring that all items are delivered via HTTPS is a critical requirement.

**Validation of Unique Code**

I assume the list of unique codes already existed, so would be possible to transport them to a high bandwidth, low latency data store, DynamoDB, to enable quick lookup and validation.

When a form was successfully submitted, a flag could be recorded against the DynamoDB record to lock this key from re-use, and has no relationship to the Census data and thus meets requirements of separation of data.

**Form submission and validation**

With the large amount of heavy lifting being done on the client side through the use of JavaScript based UI frameworks, the requirements on form submission can be greatly simplified.

By using API Gateway backed onto an AWS Lambda function, the complexity of the system under control is greatly simplified, and one key feature of the Lambda platform is it auto-scaleout capability to execute multiple Lambda functions in parallel automatically.

This function would not need to access any data store for processing as the client side JavaScript application would be able to submit the entire packet in one post.

The function would only need to validate a relatively small JSON packet for completeness and data validity, on failure return sufficient error messages for the client side application to feedback to customers.

If the submitted packet was valid, it need not access any database, but simply upload this JSON element to an S3 bucket configured for write only access.

This ensures that no function of the application ever needs to read the Census data, and is only held in the Lambda function for the duration of it being processed and either discarded or stored safely in S3.

## Conclusion

I don't want to comment on the current solution, its vendors or the approach taken by the ABS, but will conclude by saying that with

* the recent emergence of Serverless application architectures,
* the continued maturity of automated delivery solutions and
* the commoditisation of infrastructure being achieved by Cloud Computing suppliers

the possible options to deliver solutions have drastically increased.

Don't write off a lot of this new technology and the approaches behind them, while the tools and frameworks in this space might only be young, their potential has only just had the surface scratched.
